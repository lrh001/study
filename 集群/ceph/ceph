iscsi(SAN存储－块存储)
NFS(NAS存储－文件系统存储)

分布式存储，取余数

[node1]	[node2]	[node3]  OSD存储数据的主机

　　　　[mon1] [mon2] [mon3] md5sum a.txt --> 数字%3
         每个数据有３副本

      client访问mon

ceph 
     文件系统共享100PB---------mount
　　块共享100PB---------------sda
　　对象存储（写代码）例：百度云盘
    
帮助文档http://docs.ceph.org/start/intro

ceph组件

OSDs:存储设备
Monitors:集群监控组件
RBD:对象存储网关
MDSs:存放文件系统的元数据（对象存储和块存储不需要该组件）
Client:ceph客户端


ceph-deploy只能在他的工作目录下执行

步骤一：部署软件
1）在node1安装部署工具，学习工具的语法格式。
[root@node1 ~]#  yum -y install ceph-deploy
[root@node1 ~]#  ceph-deploy  --help

2）创建目录（ceph-deploy的工作目录）
[root@node1 ~]#  mkdir ceph-cluster
[root@node1 ~]#  cd ceph-cluster/

步骤二：部署Ceph集群

1）创建Ceph集群配置。
[root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3

2）给所有节点安装软件包。
[root@node1 ceph-cluster]# ceph-deploy install node1 node2 node3

3）初始化所有节点的mon服务（主机名解析必须对）
[root@node1 ceph-cluster]# ceph-deploy mon create-initial


步骤三：创建OSD

1）准备磁盘分区（node1、node2、node3都做相同操作）
[root@node1 ceph-cluster]#  parted  /dev/vdb  mklabel  gpt
[root@node1 ceph-cluster]#  parted  /dev/vdb  mkpart primary  1M  50%
[root@node1 ceph-cluster]#  parted  /dev/vdb  mkpart primary  50%  100%
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb1
[root@node1 ceph-cluster]# chown  ceph.ceph  /dev/vdb2
//这两个分区用来做存储服务器的日志journal盘
[root@node1 ceph-cluster]# vim /etc/udev/rules.d/70-vdb.rules  #设置开机自动修改磁盘所属组和所属主
ENV{DEVNAME}=="/dev/vdb1",OWNER="ceph",GROUP="ceph"
ENV{DEVNAME}=="/dev/vdb2",OWNER="ceph",GROUP="ceph"

2）初始化清空磁盘数据（仅node1操作即可）＃设置分区模式gpt
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:vdc   node1:vdd    
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:vdc   node2:vdd
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:vdc   node3:vdd   

3）创建OSD存储空间（仅node1操作即可）
[root@node1 ceph-cluster]# ceph-deploy osd create \
 node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2  
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
//一个存储设备对应一个缓存设备，缓存需要SSD，不需要很大

[root@node1 ceph-cluster]# ceph-deploy osd create \
 node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2

[root@node1 ceph-cluster]# ceph-deploy osd create \
 node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2 


创建Ceph块存储

步骤一：创建镜像

1）查看存储池。
[root@node1 ~]# ceph osd lspools
0 rbd,

2）创建镜像、查看镜像
[root@node1 ~]# rbd create demo-image --image-feature  layering --size 10G
#创建一个名为demo-image的10Ｇ镜像
[root@node1 ~]# rbd create rbd/image --image-feature  layering --size 10G
#创建一个名为image的10Ｇ镜像
[root@node1 ~]# rbd list
＃查看所有镜像
[root@node1 ~]# rbd info demo-image
＃查看demo-image镜像的详细信息








